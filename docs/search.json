[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV et al.",
    "section": "",
    "text": "Home\n    CV et al.\n  \n\n\n\n\n\n   Curriculum Vitae\n  \n    \n      ‚ÄÇDownload CV\n    \n  \n  \n    \n  \n\n\n\n   Research Statement\n  \n    \n      ‚ÄÇDownload Research Statement\n    \n  \n  \n    \n  \n\n\n\n   Teaching Statement\n  \n    \n      ‚ÄÇDownload Teaching Statement"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Roussel",
    "section": "",
    "text": "Home\n    About\n  \n\nI am a Machine Learning PhD candidate at the University of Bristol. I hold a BSc in Mathematics from Aix-Marseille University (2019) and an MSc in Applied Mathematics from UniStra (2021), advised by Prof.¬†St√©phane Labb√©.\nMy research lies at the intersection of machine learning and physical systems found in engineering, computer graphics, vision, and more. I build the next wave of generative models and simulation technologies, leveraging my deep experience in differentiable programming, in-context learning, and meta-learning to create realistic and immersive virtual worlds. Under the supervision of Dr.¬†Tom Deakin, Prof.¬†David Barton, and Prof.¬†Simon McIntosh-Smith, I combine expertise in AI, HPC, and Scientific Computing.\nBeyond research, I am committed to equity in STEM education, volunteering for outreach initiatives with CodeMakers, ExamStar, and St.¬†Theresa, among others. In my downtime, I enjoy football, playing the piano, creative coding, and building indie games.\nFeel free to explore my CV or learn more through my profile on LinkedIn. And let‚Äôs connect if you‚Äôd like to collaborate or chat!\n\n\n\n\n Education\n\n\n\n\n\n\nSept 2021 - (Anticipated Feb 2026)\n\n\nPhD in Machine Learning (Interactive AI)\n\n\n University of Bristol  Bristol, UK\n\n\n\n\n\n\n\nSept 2019 - Sept 2021\n\n\nMSc in Applied Mathematics (CSMI)\n\n\n University of Strasbourg  Strasbourg, FR\n\n\n\n\n\n\n\nNov 2017 - Jul 2019\n\n\nBSc in Mathematics\n\n\n Aix-Marseille University  Marseille, FR\n\n\n\n\n\n\n\nApr 2017 - Jun 2019\n\n\nAssociate degree in Mechatronics\n\n\n Oshima College of Technology  Oshima, JP\n\n\n\n\n\n\n\nJan 2017 - Apr 2019\n\n\nAssociate degree in Computer Science\n\n\n University of the People  Pasadena, USA\n\n\n\n\n\n\n Work Experience\n\n\n\n\n\n\nJan 2022 - Present\n\n\nTeaching Assistant\n\n\n University of Bristol  Bristol, UK\n\n\n\n\n\n\n\nJun 2024 - Sept 2024\n\n\nData Science Internship\n\n\n SLB (Schlumberger)  Abingdon, UK\n\n\n\n\n\n\n\nMay 2022 - Aug 2022\n\n\nPhD Summer Projects\n\n\n HPC Research Group & Bristol Robotics Lab.  Bristol, UK\n\n\n\n\n\n\n\nFeb 2021 - Jul 2021\n\n\nMSc Internship\n\n\n Laboratoire Jacques-Louis Lions (Sorbonne Universit√©)  Paris, FR\n\n\n\n\n\n\n\nJun 2020 - Aug 2020\n\n\nMSc Internship\n\n\n Institut de Recherche Math√©matiques Avanc√©es (IRMA)  Strasbourg, FR\n\n\n\n\n Hobbies\n\n\n\n\n\n\n\nI enjoy all things computer hardware, software and games: from old classics to AAA‚Äôs like Titanfall. Nothing like a first-person shooter to evade your troubles.\n\n\n\n\n\n\n\nI‚Äôm pretty sure there exists a dimension in which I am a moviemaker or a movie score composer √† la Giacchino !\n\n\n\n\n\n\n\n\n\nMy old Casiotone CT-S100. I recently got back to making music. Hopefully some of my pieces will feature here soon.\n\n\n\n\n\n\n\nFootball is life. I play weekly in one of Bristol‚Äôs minor leagues. Call me if you‚Äôre putting together a game.\n\n\n\n\n\n\n Address\n\n\n\n\n Contact me"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Roussel.",
    "section": "",
    "text": "Hello there! I‚Äôm Roussel Desmond Nzoyem (pronounced zo-e-m). To learn more about me, go to the About and CV pages. You‚Äôll find work I‚Äôve contributed to in Projects and Publications. For a nice read, go to the Blog section where you‚Äôll find interesting stories.\n\n\n\n\n Selected Stories*\n\n\n*Stay up to date by signing up for my newsletter. \n\n Linear Attention in JAX Exploring efficient attention mechanisms through JAX implementation Coming soon\n\n\n A Parallel Theory of the Causal Mind Investigating how our minds process causality through parallel mechanisms\n\n\n Welcome to My Brand-New Website! An introduction to this space where I share research insights, technical explorations, and thoughts on AI\n\n\n Mesh-Free Simulation in the Age of Big Data How modern mesh-free methods are revolutionizing computational physics and engineering Coming soon\n\n\n\n\n\n Latest News*\n\n\n24 Jun.¬†2025: I am co-organizing a two-days conference focused on embedding domain-specific knowledge in ML models, physical and mathematical methods for AL/ML development, and applications. ‚òû Programme\n\n\n10 Jun.¬†2025: I‚Äôm giving a flash talk the UoB‚Äôs SciML in the Faculty of Engineering Workshop on a new adaptation rule I am extremely excited about: Weight-Space Linear Recurrent Neural Networks. ‚òû Slides\n\n\n19 May. 2025: I was in Montreal for the CRM summer school on the Mathematical Foundations of Data Science. I also attended the workshop on Optimization and learning: theory and applications !\n\n\n22 Apr.¬†2025: I‚Äôm attending ICLR‚Äô25 in Singapore and presenting lots of stuff. This time again, I‚Äôm looking to connect and have as much fun as possible. ‚òû LinkedIn Post\n\n\n06 Mar.¬†2025: Our latest paper MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning was unanimously accepted into the SCOPE workshop @ ICLR 2025. Looking forward to seeing everyone in Singapore !\n\n\n13 Feb.¬†2025: I gave a talk at the University of Bristol‚Äôs EPS Seminar Series and the Engineering Design Society. It‚Äôs about Neural Context Flows and follow-up work on foundational models for science. ‚òû Slides\n\n\n22 Jan.¬†2025: I‚Äôm delighted to announce that our paper Neural Context Flows has been accepted to ICLR 2025. More info to come, including a blogpost with code and weights !\n\n \n\n*More news!\n\n \n\n\n\n\n07 May. 2024: I‚Äôm attending ICLR‚Äô24 in Vienna and presenting a poster at the workshop on AI4DifferentialEquations in Science.\n\n\n17 Apr.¬†2024: My profile now appears on the Engineering Includes Me wall in the University of Bristol‚Äôs iconic Queen‚Äôs Building. ‚òû Blog Post\n\n\n13 Mar.¬†2024: I‚Äôm going to the Turing Institute to attend the launch of their Probabilistic Programming theme.\n\n\n20 Nov.¬†2023 I‚Äôm giving a talk at the 2nd workshop on Physics Enhancing Machine Learning in Applied Mechanics, held at the Institute of Physics in London, UK. ‚òû Slides.\n\n\n13 Nov.¬†2023: Our paper on mesh-free differential programming for optimal control is out during SuperComputing‚Äô23. ‚òû Read here.\n\n\n18 Sep.¬†2023: I‚Äôm attending the 175th European Study Group with Industry (ESGI) in Berlin, Germany.\n\n\n01 Aug.¬†2023: I‚Äôve pre-released version 0.1.4 of Updes: a mesh-free differentiable software for quick experimentation with a variety of PDEs. ‚òû Repo.\n\n\n10 Mar.¬†2023 I gave a talk at CMU Africa on emerging techniques and applications of graph neural networks. ‚òû Recording.\n\n\n01 Dec.¬†2022: I began my research on AI, HPC, and Scientific Machine Learning for mesh-free simulations.\n\n\n10 Oct.¬†2021: I moved to Bristol for a PhD in the Interactive AI CDT.\n\n\n\n\n\n\n\n Selected Publications*\n\n\n\n\n*Please check my Google Scholar for more.\n\n\nAAAI 2026 Student Abstract & Poster Program\n\n\n\nLanguage Models Do Not Embed Numbers Continuously\n\n\nA Davies, RD Nzoyem, N Ajmeri\n\n\n\nPDF Code\n\n\n\n\n\nArXiv 2025\n\n\n\nWeight-Space Linear Recurrent Neural Networks\n\n\nRD Nzoyem, N Keshtmand, E Crespo Fernandez, I Tsayem, R Santos-Rodriguez, DAW Barton, T Deakin\n\n\nAbstract PDF Code\n\n\n\n\n\nSCOPE@ICLR 2025\n\n\n\nMixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning\n\n\nRD Nzoyem, G Stevens, A Sahota, DAW Barton, T Deakin\n\n\nAbstract PDF Code\n\n\n\n\n\nCoLLAs 2025\n\n\n\nReevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation\n\n\nRD Nzoyem, DAW Barton, T Deakin\n\n\nAbstract PDF\n\n\n\n\n\nICLR 2025\n\n\n\nNeural Context Flows for Meta-Learning of Dynamical Systems\n\n\nRD Nzoyem, DAW Barton, T Deakin\n\n\nAbstract PDF Code\n\n\n\n\n\nSC‚ÄôW 2023\n\n\n\nA Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control\n\n\nRD Nzoyem, DAW Barton, T Deakin\n\nAbstract PDF\n\n\n\n\n\n\n\n\n Recent Services\n\n\nReviewer:\n\n\nEuro-PAR‚Äô24 ICLR‚Äô25 ICLR‚Äô26 ICML‚Äô25 (Top Reviewer) NeurIPS‚Äô25 TMLR SCOPE@ICLR 2025\n\n\nConferences:\n\n\nJoint UKRI CDT Conference in Artificial Intelligence, Machine Learning & Advanced Computing Co-organizer | June 2025, Bristol, UK\n\n\nThe Interactive AI Spring Research Conference Co-organizer | March 2024, Bristol, UK\n\n\n\n\n\n Recent Talks\n\n\nWeight-Space Linear Recurrent Neural Networks Workshop on Scientific Machine Learning, University of Bristol, UK | June 2025\n\n\nNeural Context Flows for Meta-Learning of Dynamical Systems EPS Seminar Series, University of Bristol, UK | Feb 2025\n\n\nDifferentiable Programming for Mesh-Free Fluid Control Physics Enhancing ML in Applied Mechanics, Institute of Physics, London | Nov 2024\n\n\nEmerging Techniques and Applications of Graph Neural Networks Graduate Student Seminar, CMU Africa, Kigali, Rwanda | Mar 2023\n\n\n\n\n\n\n\n\n Reach out"
  },
  {
    "objectID": "projects/phifem/index.html",
    "href": "projects/phifem/index.html",
    "title": "œï-FEM",
    "section": "",
    "text": "In recent years, numerical models using the Finite Elements Method (FEM) to simulate the soft tissue mechanisms of the human body have attracted a great interest. In the context of computer-assisted surgery, the simulation method should be quick, precise, and patient-specific. This work develops a new immersed boundary method named œÜ-FEM to address those issues.\nSoftware: - Fenics - SOFA"
  },
  {
    "objectID": "projects/ncflow/index.html",
    "href": "projects/ncflow/index.html",
    "title": "Neural Context Flow",
    "section": "",
    "text": "Our goal is to build a foundational machine learning model for all of science. Neural Context Flow is the first steps towards that goal, which generalizes dynamical system learning to out of distribution parameters.\nSoftware stack: JAX, Equinox, Diffrax and more.\nGitHub: üëâ Neural Context Flow"
  },
  {
    "objectID": "projects/vnetcancer/index.html",
    "href": "projects/vnetcancer/index.html",
    "title": "VnetAgainstCancer",
    "section": "",
    "text": "We solved a computerized tomography inverse problem: given the signal on the boundaries of an organ, we want to rebuild the density map of the organ, hence detecting abnormally high density zones (which are potential indicators of early-onset cancer). Our main tasks were: simulating the radiative transfer equation; using Convolutional Neural Network to solve the related inverse problem; using a V-Net to improve accuracy and recreate the complete density map.\nSoftware stack: - Transfer - Eigen - TensorFlow\nRead an abstract of our report here."
  },
  {
    "objectID": "publications/nzoyem2020simulation.html",
    "href": "publications/nzoyem2020simulation.html",
    "title": "Simulation 2D de l‚Äô√©quation du transfert radiatif et reconstruction de la densit√© par un r√©seau de neurones",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2020simulation.html#citation-apa",
    "href": "publications/nzoyem2020simulation.html#citation-apa",
    "title": "Simulation 2D de l‚Äô√©quation du transfert radiatif et reconstruction de la densit√© par un r√©seau de neurones",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R., Franck, E., Navoret, L., Vigon, V., & PRUD‚ÄôHOMME, C. (2020). Simulation 2D de l‚Äôequation du transfert radiatif et reconstruction de la densit√© par un r√©seau de neurones."
  },
  {
    "objectID": "publications/nzoyem2020simulation.html#abstract",
    "href": "publications/nzoyem2020simulation.html#abstract",
    "title": "Simulation 2D de l‚Äô√©quation du transfert radiatif et reconstruction de la densit√© par un r√©seau de neurones",
    "section": "Abstract",
    "text": "Abstract\nEn 2015, le r√©seau de neurones vainqueur de l‚ÄôILSVRC1 obtient une pr√©cision de 97.3 % ce qui conduit les chercheurs √† postuler que les machines peuvent identifier les objets dans des images mieux que les humains. Depuis lors, le domaine du Machine Learning a continu√© √† prendre de l‚Äôampleur. Aujourd‚Äôhui ses applications se multiplient dans plusieurs secteurs d‚Äôactivit√© parmi lesquelles l‚Äôautomobile, la finance, le divertissement, et plus important, celui de la sant√© √† travers l‚Äôimagerie m√©dicale.\nLes tumeurs ont des propri√©t√©s optiques diff√©rentes des tissus qui les entourent2. √âtant donn√© un domaine avec un faisceau lumineux qui s‚Äôy propage, reconstruire sa densit√© √† l‚Äôaide du signal temporel mesur√© sur ses bords constitue un probl√®me inverse. Les probl√®mes inverses sont tr√®s importants en sciences math√©matiques et ont des applications vari√©es en imagerie m√©dicale, radar, vision, etc. Ils sont malheureusement tr√®s difficiles √† r√©soudre car ils n√©cessitent l‚Äôutilisation d‚Äôalgorithmes d‚Äôoptimisation avanc√©s. Les r√©seaux de neurones artificiels se pr√©sente comme une m√©thode potentiellement moins couteuse mais plus rapide.\nGrace √† son unit√© mixte de recherche IRMA, l‚ÄôUFR de math√©matique et d‚Äôinformatique de l‚ÄôUniversit√© de Strasbourg est un p√¥le de recherche en math√©matiques appliqu√©es. √Ä travers ses √©quipes MOCO et Probabilit√©s, l‚ÄôIRMA s‚Äôint√©resse aux probl√©matiques de mod√©lisation des EDP et de Machine Learning, raison pour laquelle j‚Äôai choisi d‚Äôy effectuer mon stage de master 1 CSMI3. Au cours de ce stage (du 15 juin au 15 ao√ªt 2020), j‚Äôai pu m‚Äôint√©resser au probl√®me inverse de reconstruction de la densit√© d‚Äôun domaine par un r√©seau de neurones convolutif (CNN).\nCe stage a √©t√© suivi par les enseignants-chercheurs MM. Emmanuel FRANCK, Laurent NAVORET, et Vincent VIGON et s‚Äôinscrit dans la continuation d‚Äôun projet (encadr√© par la m√™me √©quipe) qui s‚Äôest d√©roul√© du 19 mars au 28 mai 2020. Le projet consistait en la simulation 1D d‚Äôun sch√©ma de ¬´ splitting ¬ª pour le mod√®le P1 de l‚Äô√©quation du transfert radiatif coupl√© avec la mati√®re. Le stage quant √† lui a essentiellement consist√© en la simulation du m√™me sch√©ma en 2D, et en la reconstruction de la densit√© par un CNN. Plus g√©n√©ralement, ce stage a √©t√© l‚Äôopportunit√© pour moi d‚Äôapprendre sur les EDP et l‚Äôapprentissage profond tout en me familiarisant avec l‚Äôinterface de programmation de la librairie de r√©seaux de neurones Keras.\nEn vue de rendre compte de mani√®re fid√®le des deux mois pass√©s au sein de l‚ÄôIRMA, il apparait logique de pr√©senter en titre de pr√©ambule le cadre du stage et son environnement technique. Ensuite il s‚Äôagira de pr√©senter les diff√©rentes missions et t√¢ches qui j‚Äôai pu effectuer. Enfin je pr√©senterais un bilan du stage, en incluant les diff√©rents apports et enseignements que j‚Äôai pu en tirer."
  },
  {
    "objectID": "publications/nzoyem2020simulation.html#footnotes",
    "href": "publications/nzoyem2020simulation.html#footnotes",
    "title": "Simulation 2D de l‚Äô√©quation du transfert radiatif et reconstruction de la densit√© par un r√©seau de neurones",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImageNet Large Scale Visual Recognition Challenge‚Ü©Ô∏é\nLes tissus canc√©reux sont g√©n√©ralement plus denses que les tissus sains.‚Ü©Ô∏é\nCalcul Scientifique et Math√©matiques de l‚ÄôInformation‚Ü©Ô∏é"
  },
  {
    "objectID": "publications/nzoyem2025foundational.html",
    "href": "publications/nzoyem2025foundational.html",
    "title": "MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2025foundational.html#citation-apa",
    "href": "publications/nzoyem2025foundational.html#citation-apa",
    "title": "MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R. D., Stevens, G., Sahota, A., Barton, D. A. W., & Deakin, T. (2025). MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning. First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models. Retrieved from https://openreview.net/forum?id=NteuHm0UXw"
  },
  {
    "objectID": "publications/nzoyem2025foundational.html#abstract",
    "href": "publications/nzoyem2025foundational.html#abstract",
    "title": "MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning",
    "section": "Abstract",
    "text": "Abstract\nAs foundational models reshape scientific discovery, a bottleneck persists in dynamical system reconstruction (DSR): the ability to learn across system hierarchies. Many meta-learning approaches have been applied successfully to single systems, but falter when confronted with sparse, loosely related datasets. Mixture of Experts (MoE) offers a natural paradigm to address these challenges. Despite their potential, naive MoEs are inadequate for the nuanced demands of hierarchical DSR, largely due to their gradient descent-based gating update mechanism which leads to slow updates and conflicted routing during training. To overcome this limitation, we introduce MixER: Mixture of Expert Reconstructors, a novel sparse top-1 MoE layer employing a custom gating update algorithm based on K-means and least squares. Extensive experiments validate MixER‚Äôs capabilities, demonstrating efficient training and scalability to systems of up to ten parametric ordinary differential equations. However, further analysis indicates that our layer underperforms state-of-the-art meta-learners in high-data regimes, particularly when each expert is constrained to process only a fraction of a dataset composed of highly related data points."
  },
  {
    "objectID": "publications/nzoyem2025reevaluating.html",
    "href": "publications/nzoyem2025reevaluating.html",
    "title": "Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2025reevaluating.html#citation-apa",
    "href": "publications/nzoyem2025reevaluating.html#citation-apa",
    "title": "Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R. D., Barton, D. A. W., & Deakin, T. (2025). Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation. Retrieved from https://openreview.net/forum?id=TzxHreJ1og"
  },
  {
    "objectID": "publications/nzoyem2025reevaluating.html#abstract",
    "href": "publications/nzoyem2025reevaluating.html#abstract",
    "title": "Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation",
    "section": "Abstract",
    "text": "Abstract\nContextual Self-Modulation (CSM) (Nzoyem et al.¬†2025) is a potent regularization mechanism for Neural Context Flows (NCFs) which demonstrates powerful meta-learning on physical systems. However, CSM has limitations in its applicability across different modalities and in high-data regimes. In this work, we introduce two extensions: CSM which expands CSM to infinite-dimensional variations by embedding the contexts into a function space, and StochasticNCF which improves scalability by providing a low-cost approximation of meta-gradient updates through a sampled set of nearest environments. These extensions are demonstrated through comprehensive experimentation on a range of tasks, including dynamical systems, computer vision challenges, and curve fitting problems. Additionally, we incorporate higher-order Taylor expansions via Taylor-Mode automatic differentiation, revealing that higher-order approximations do not necessarily enhance generalization. Finally, we demonstrate how CSM can be integrated into other meta-learning frameworks with FlashCAVIA, a computationally efficient extension of the CAVIA meta-learning framework (Zintgraf et al.¬†2019). Together, these contributions highlight the significant benefits of CSM and indicate that its strengths in meta-learning and out-of-distribution tasks are particularly well-suited to physical systems."
  },
  {
    "objectID": "publications/nzoyem2022accelerating.html",
    "href": "publications/nzoyem2022accelerating.html",
    "title": "Accelerating Algebraic Multigrid Using Machine Learning",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2022accelerating.html#citation-apa",
    "href": "publications/nzoyem2022accelerating.html#citation-apa",
    "title": "Accelerating Algebraic Multigrid Using Machine Learning",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R., Louw, T., McIntosh-Smith, S. & Deakin, T., (2022). Accelerating Algebraic Multigrid Using Machine Learning: Application to problems originating from fluid simulation."
  },
  {
    "objectID": "publications/nzoyem2022accelerating.html#abstract",
    "href": "publications/nzoyem2022accelerating.html#abstract",
    "title": "Accelerating Algebraic Multigrid Using Machine Learning",
    "section": "Abstract",
    "text": "Abstract\nHigh-fidelity fluid simulations require solving extremely large and sparse linear systems, often in- volving millions of unknowns. With its fast convergence properties, Algebraic Multigrid (AMG) is the method of choice in several application areas. This said, classical AMG suffers from a number of issues, calling Machine Learning (ML) to the rescue. Recent years have seen a flourish of ML tactics to accelerate AMG. However, published work tends to focus on small and unrepresentative problems. Moreover, these methods tend to be developed dissociatively, not involving human end users in the process.\nWith insights from potential users, we aim to build a ML-augmented AMG framework that is: (i) robust i.e.¬†reusable in a variety of problems arising from fluid simulation; (ii) computationally cheap i.e.¬†converges faster than the classical AMG; (iii) works for structured as well as unstructured grids; (iv) works for small systems as well as large systems, while taking advantage of their sparsity; and (v) scalable i.e.¬†data- and model-parallelisable with efficient memory management.\nThe main contributions of this project are as follows: (1) a Graph Neural Network methodology for learning prolongation operators, built around DGL and PyTorch; (2) a Cloud Formation stack on AWS to run experiments, containing all the dependencies our implementation requires; (3) ex- periments indicating that our current AI model is better than the classical approach in about 20% of cases; and (4) a survey that, among other remarks, endeared participants to using (AI-enabled) AMG for their various problems.\nOur work entails much efficient usage of HPC resources. By noticing that ML can be leveraged not only when solving linear systems, we pave the way for ML to be efficiently used in other parts of the high-fidelity simulation pipeline. Moreover, the consideration of users and their needs is a positive step towards broader Design Space Exploration of prolongation operators, model hyperparameters, and physical parameters influencing mechanical engines in operation."
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html",
    "href": "posts/parallel-mind-theory/index.html",
    "title": "A parallel theory of the causal mind",
    "section": "",
    "text": "I‚Äôve always been fascinated by theories of the mind and how to fully replicate it with artificial general intelligence. Lately, the philosophical nature of this problem is one that I‚Äôve found myself pondering over a lot. Hoping not to get into too much detail, I‚Äôll talk about two technologies I believe are key to deciphering how the human mind works: Neural Ordinary Differential Equations, and Time-Parallel Computing. This article is based on ideas I collected from John Searle‚Äôs course Philosophy of Mind, and Malcolm Gladwell‚Äôs best-selling book Blink."
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html#the-mind-body-problem",
    "href": "posts/parallel-mind-theory/index.html#the-mind-body-problem",
    "title": "A parallel theory of the causal mind",
    "section": "1 The mind-body problem",
    "text": "1 The mind-body problem\nThe mysteries of consciousness and the soul are some of the most challenging questions of our time. To tackle these questions, the Cartesian view postulates the existence of two realms: a real of Mind (whose essence is the ‚Äúthinking‚Äù), and a real of Body (or the physical)1. The mind is indivisible, undoubtable (hence the famous phrase ‚ÄúI think, therefore I am.‚Äù), and commands the body. This fomulation, whose ideas we carry to this day, is an apt introduction to the dualist view, despite being a gross oversimplification of Ren√© Descartes‚Äô legacy.\nThe dualist formulation dated back to the 17th century and is convenient for its time, as it leaves the realm of Body to science, and the realm of Mind to religion. Perfect if you‚Äôre a scientist trying to conduct thought-provoking research under the Inquisition. Descartes‚Äôs theory is appealing, but it leaves us with one big problem: how does the mind influence the brain? How are the two realms connected? Descartes‚Äôs answer to this Mind-Body puzzle was unconvincing.\nOver the centuries, several schools of thought have sought an answer to the puzzle, at times only considering one of the two realms. The main groups being idealism and behaviourism for Mind and Body, respectively. This is beautifully explained by John Searle in his course Philosophy of Mind. It would appear that the advent of computers and the emergence of the cross-disciplinary field of cognitive science is the key to this puzzle."
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html#the-bottom-up-mind-body-causality",
    "href": "posts/parallel-mind-theory/index.html#the-bottom-up-mind-body-causality",
    "title": "A parallel theory of the causal mind",
    "section": "2 The bottom-up mind-body causality",
    "text": "2 The bottom-up mind-body causality\nComputers function by means of algorithms: carefully established instructions relating an input to an output. If one were to think of the brain (and daringly the mind too) as a computer, then the causal relation between the brain and the mind would have to be at the heart of such formalism. That is the essence of cognitive science.\nWhen you feel pain, hunger or joy, there‚Äôs no doubt these are neurobiological processes in your brain firing. This suggests that the mind reacts to the behaviour of the world; it is caused by brain processes. On the other hand, when you decide to raise your hand and it goes up, or when you go from being unhappy to being ecstatic, neuroscientists can clearly observe a change in the physical disposition of your brain. In other words, your mind is a state of your brain, a feature.\nAs explained by Searle, the brain causes the mind, and the mind is a feature of the brain. This would appear paradoxical, but no, it isn‚Äôt. It makes sense if we think of the mind as made up of the higher-level processes compared to what we measure in the brain. For instance, when you decide to raise your hand and it goes up, there are two ways to interpret what happened: (1) neurons activated somewhere in your brain, sent a signal that travelled to your muscles, then causing your hand to go up (the low-level processes); (2) you had a thought, and you observed the materialisation of it as a hand in the air (the high-level process). Both interpretations are equally and simultaneously true.\nHow do higher-level systemic features emerge from low-level individual characteristics? This is the research question we‚Äôve been after. We‚Äôve turned an abstract philosophical puzzle into a very materialistic one; a scientific problem that includes the mind (unlike Descartes‚Äô original formulation). Mind-blowing behaviour emerging from simplistic elementary processes is widespread in nature. My favourite example is the swirl of Starling birds.\n\n\n\n\n\n\n\nNote\n\n\n\nThis idea isn‚Äôt particularly groundbreaking. I‚Äôve recently heard, on the Joy of Why podcast, about Professor Anil Seth and his pursuit of similar research questions at the University of Sussex."
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html#a-new-model-for-the-mind-and-the-brain",
    "href": "posts/parallel-mind-theory/index.html#a-new-model-for-the-mind-and-the-brain",
    "title": "A parallel theory of the causal mind",
    "section": "3 A new model for the mind and the brain",
    "text": "3 A new model for the mind and the brain\nAs an applied mathematician, the propagation of brain signals and the elusive bottom-up causality between low- and high-level processes remind me of two specific tools: differential equations and neural networks. I liken the causality and evolutionary nature of brain processes to ordinary differential equations (ODEs) as they describe individual dynamics. I then ascribe the compositionality of simple behaviour into larger features ‚Äî difficult to account for with our current understanding of neuroscience ‚Äî to deep neural networks.\nAs it turns out, those two concepts have been merged in what is now known as Neural ODEs. Our model for the mind-body relation is formulated as follows \\[\\begin{align}\n\\frac{\\text{d} y}{\\text{d} t}(t) &= f_{\\theta}(y,t) \\qquad t \\in ]t_0, t_f [ \\\\\ny(t_0) &= y_0 \\\\\nz(t) &= g_{\\theta'}(\\theta, y, y_0, t, t_0, t_f),\n\\end{align}\\] where \\(y\\) represents the physical signal transported and processed inside the brain. The transformative function \\(f_{\\theta}\\) ‚Äî where the learnable parameters \\(\\theta\\) indicate a deep neural network2 ‚Äî dictates how such continuous processing occurs. The mysterious readout function \\(g_{\\theta'}\\) tells us how a brain stimulus \\(y_0\\in \\mathbb{R}^b\\) turns into a global mind feature \\(z \\in \\mathbb{R}^m\\) (with \\(m\\gg b\\), i.e.¬†potentially lots more elements involved in forming a mind compared to processing an elementary brain signal).\n\n\n\n\nFigure 1: Illustration of the Neural ODE model for the computational mind/brain relation\n\n\nThe Neural ODE paradigm has found breathtaking success, particularly as a drop-in replacement for ResNets. But neural networks today can grow to extremely large sizes with billions of parameters in the weights \\(\\theta\\). They are energy and computationally inefficient compared to the human brain from which they are inspired. What if this network didn‚Äôt have a fixed structure? What if neurons could dynamically adapt to the task at hand? These are the questions that Liquid Neural Networks attempt to answer. They were inspired by the efficiency of the worm‚Äôs brain, made up of merely 300 neurons; and have shown jaw-dropping performance in autonomous driving. I believe Neural ODEs and Liquid Neural Networks could be combined within a Liquid Neural ODE paradigm, thus unlocking the secrets of consciousness."
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html#accounting-for-the-subconscious",
    "href": "posts/parallel-mind-theory/index.html#accounting-for-the-subconscious",
    "title": "A parallel theory of the causal mind",
    "section": "4 Accounting for the subconscious",
    "text": "4 Accounting for the subconscious\nAs a full-time daydreamer, I believe any theory of the mind must account for the subconscious; although not necessarily as envisioned by Sigmund Freud, since I don‚Äôt believe his deeply abstract (and quite frightening) but brillant ideas are indispensable to replicating the human mind. That is why I think there‚Äôs a second computer at work in the back of our minds, even when we are awake. The same computer that quickly processes information and lets a car driver avoid pedestrians in an emergency situation. The kind of computer that shows us how we‚Äôre all carrying implicit biases. The same one we make use of on first impressions. The computer in the background that professionals use to identify talent without even knowing such things are happening subconsciously.\nIf those ideas sound familiar, that‚Äôs because they are collected from Malcolm Gladwell‚Äôs bestselling book Blink. Gladwell repeatedly uses the terminology thin slicing. He describes a computer that, given sufficient experience in a domain, discards all useless information to make the quickest decisions for our assumed benefit. The moral of the book is: listen to your inner voice, but know when to ignore it. Besides the discarding of useless information, I believe thin slicing happens so quickly because of the superior computational performance of that second computer.\nSo, how can this idea complement the Neural ODE (which represents the conscious mind in our model)? Well, we parallelise it. The model above, given an heterogenous input vector \\(y\\), should split the time-horizon \\(]t_0, t_f[\\) and distribute the chunks to much smaller units of compute in order to return faster results at competitive accuracy. This can be achieved if two time domains are considered, one coarse on which guesses for \\(y\\) and \\(z\\) are refined, and one fine on which brain units work in parallel. We can interpret this parallelisation as signals coming from various senses at different times, and processed by different units within the brain. What I‚Äôm describing is combined data- and time-parallelisation. We train the Neural ODE on a fine time domain with a large \\(y\\) containing all possible information, but under subconscious thin slicing conditions, we make sure inference happens on the coarse time domain with a smaller less informative \\(y\\) in successive stages.\nSeveral works have investigated parallel ideas on Neural ODEs, namely Gunther et al.‚Äôs and Massaroli et al.‚Äôs. However, they only considered time (or layer) parallelism. Moreover, they focus on the training, and show very limited interest for inference. Combining data and time for quick but accurate inference is a dream I hope to achieve before then end of my PhD in September 2025."
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html#closing-thoughts",
    "href": "posts/parallel-mind-theory/index.html#closing-thoughts",
    "title": "A parallel theory of the causal mind",
    "section": "5 Closing thoughts",
    "text": "5 Closing thoughts\nI‚Äôm currently investigating the feasibility of a data-time parallel Neural ODE with applications to PDE simulation. How would we test such an idea on the brain? I have no clue yet. What I know is that I can start with my brain, like in the famous Chinese Room argument3 (as Searle says, ‚Äúalways test the theory on yourself first‚Äù).\nIf your mind finds itself drawn to these ideas, then maybe they‚Äôre valid. In that case, send me a message. If you‚Äôve heard these same ideas somewhere else and think I‚Äôm wasting my time or that there‚Äôs room for collaboration, then please do reach out. If this sounds like complete nonsense to you, then maybe it is. It shouldn‚Äôt stop us from pursuing the truth though, right ?\nUsing your GitHub account, please comment below for any insights you might have. And as always, thanks for reading !"
  },
  {
    "objectID": "posts/parallel-mind-theory/index.html#footnotes",
    "href": "posts/parallel-mind-theory/index.html#footnotes",
    "title": "A parallel theory of the causal mind",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the best I can do the define what the mind is, as opposed to what the body is. In this article, the body is reduced to the brain without loss of generality.‚Ü©Ô∏é\nIn this model, \\(\\theta\\) is optional in cases where neurobiological input-output relations are fully understood. \\(\\theta'\\) on the other hand, is not optional.‚Ü©Ô∏é\nWhile it concedes the idea of a computational brain, this argument proves computation alone cannot explain the mind. In our model, we account for that with our readout function \\(g_{\\theta'}\\).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my brand-new website !",
    "section": "",
    "text": "Welcome! This is my very first post on this blog.\nI‚Äôve never really owned a blog, even though it has been on my mind for more than 2 years now. Now that I‚Äôve got a brand-new portfolio website, I feel like I‚Äôm never going to get a better opportunity than this to flesh out some of my ideas.\nPrevious iterations of my personal websites might not be accessible anymore, but snapshoots can be found below.\n\n\n\n\n\n\nVersion 1 built with Jekyll based on the Alembic template. This version is now archived on GitHub.\n\n\n\n\n\n\n\n\n\nVersion 2 built with ReactJS based on Soumyajit‚Äôs template. This version is still live, although not for long.\n\n\n\n\n\n\n\n\n\nVersion 3 built with Quarto and using the yeti (light) and superhero (dark) themes. This is the site you‚Äôre currently browsing.\n\n\n\n\n\nI cannot say what was wrong with versions 1 and 2. My biggest concern was that I couldn‚Äôt write code like in iPython notebooks. The coding feature is something I value a lot for Data Science tutorials. The current iteration (version 3) is built on Quarto which fully supports in-line code cells in Python and Julia, my top 2 scripting languages. It offered a template more gorgeous than anything that I found elsewhere.\nNow that you know my reasons for making these changes, you can tell me which version you think is best. You can react below with üëç for version 1, üéâ for version 2, or ‚ù§Ô∏è for version 3. The comment section is handled via Giscus, so you need to create a GitHub account to be able to react or leave a comment. It‚Äôs super easy though.\nPlease go ahead and tell me which one‚Äôs preferable so that I can incorporate the best of what that version has to offer. As always, thanks for reading !"
  },
  {
    "objectID": "publications/nzoyem2025weightspace.html",
    "href": "publications/nzoyem2025weightspace.html",
    "title": "Weight-Space Linear Recurrent Neural Networks",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2025weightspace.html#citation-apa",
    "href": "publications/nzoyem2025weightspace.html#citation-apa",
    "title": "Weight-Space Linear Recurrent Neural Networks",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R. D., Keshtmand, N., Tsayem, I., Barton, D. A. W., & Deakin, T. (2025). Weight-Space Linear Recurrent Neural Networks. ArXiv Preprint. Retrieved from https://arxiv.org/abs/2506.01153"
  },
  {
    "objectID": "publications/nzoyem2025weightspace.html#abstract",
    "href": "publications/nzoyem2025weightspace.html#abstract",
    "title": "Weight-Space Linear Recurrent Neural Networks",
    "section": "Abstract",
    "text": "Abstract\nWe introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet powerful framework that unifies weight-space learning with linear recurrence to redefine sequence modeling. Unlike conventional recurrent neural networks (RNNs) which collapse temporal dynamics into fixed-dimensional hidden states, WARP explicitly parametrizes the hidden state as the weights of a distinct root neural network. This formulation promotes higher-resolution memory, gradient-free adaptation at test-time, and seamless integration of domain-specific physical priors. Empirical validation shows that WARP matches or surpasses state-of-the-art baselines on diverse classification tasks, spanning synthetic benchmarks to real-world datasets. Furthermore, extensive experiments across sequential image completion, dynamical system reconstruction, and multivariate time series forecasting demonstrate its expressiveness and generalization capabilities. Critically, WARP‚Äôs weight trajectories offer valuable insights into the model‚Äôs inner workings. Ablation studies confirm the architectural necessity of key components, solidifying weight-space linear RNNs as a transformative paradigm for adaptive machine intelligence."
  },
  {
    "objectID": "publications/nzoyem2021fracturation.html",
    "href": "publications/nzoyem2021fracturation.html",
    "title": "Fracturation de floes de glace par percussion dans un mod√®le granulaire",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2021fracturation.html#citation-apa",
    "href": "publications/nzoyem2021fracturation.html#citation-apa",
    "title": "Fracturation de floes de glace par percussion dans un mod√®le granulaire",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R., Labb√©, S. & Prud‚Äôhomme, C. (2021). Fracturation de floes de glace par percussion dans un mod√®le granulaire."
  },
  {
    "objectID": "publications/nzoyem2021fracturation.html#abstract",
    "href": "publications/nzoyem2021fracturation.html#abstract",
    "title": "Fracturation de floes de glace par percussion dans un mod√®le granulaire",
    "section": "Abstract",
    "text": "Abstract\nThe rapid shrinking of the Arctic ice cap these last decades is seen as one of the most striking manifestations of global warming. This decrease in size opens way to new opportunities, namely the creation of routes beneficial to the industrial sector. The second major consequence of this observation that we need to include the Marginal Ice Zone (MIZ) into climate prediction models. In 2015, Rabatel et al.1 developed a sophisticated model for the dynamics of rigid ice floes. The model was later enhanced by Balasoiu in 2020 when he considered the ice floe not as rigid, but as an elastic material modeled by a mass‚Äêspring‚Äêdamper lattice2. Our main goal in this report is to study what happens when two or more ice floes collide (percussion, fracture, etc.), both in 1D and in 2D."
  },
  {
    "objectID": "publications/nzoyem2021fracturation.html#footnotes",
    "href": "publications/nzoyem2021fracturation.html#footnotes",
    "title": "Fracturation de floes de glace par percussion dans un mod√®le granulaire",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMatthias RABATEL et al.¬†¬´ Dynamics of an assembly of rigid ice floes ¬ª. In : Journal of Geophysical Research : Oceans 120.9 (2015), p.¬†5887‚Äê5909.‚Ü©Ô∏é\nDimitri BALASOIU. ¬´ Mod√©lisation et simulation du comportement m√©canique de floes de glace ¬ª. Theses. Universit√© Grenoble Alpes [2020‚Äê‚Ä¶.], oct. 2020. URL : https://tel. archives-ouvertes.fr/tel-03116132.‚Ü©Ô∏é"
  },
  {
    "objectID": "publications/nzoyem2023comparison.html",
    "href": "publications/nzoyem2023comparison.html",
    "title": "A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2023comparison.html#citation-apa",
    "href": "publications/nzoyem2023comparison.html#citation-apa",
    "title": "A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R., Barton, D., & Deakin, T., (2023). A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints."
  },
  {
    "objectID": "publications/nzoyem2023comparison.html#abstract",
    "href": "publications/nzoyem2023comparison.html#abstract",
    "title": "A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints",
    "section": "Abstract",
    "text": "Abstract\nThe field of Optimal Control under Partial Differential Equations (PDE) constraints is rapidly changing under the influence of Deep Learning and the accompanying automatic differentiation libraries. Novel techniques like Physics-Informed Neural Networks (PINNs) and Differentiable Programming (DP) are to be contrasted with established numerical schemes like Direct-Adjoint Looping (DAL). We present a comprehensive comparison of DAL, PINN, and DP using a general-purpose mesh-free differentiable PDE solver based on Radial Basis Functions. Under Laplace and Navier-Stokes equations, we found DP to be extremely effective as it produces the most accurate gradients; thriving even when DAL fails and PINNs struggle. Additionally, we provide a detailed benchmark highlighting the limited conditions under which any of those methods can be efficiently used. Our work provides a guide to Optimal Control practitioners and connects them further to the Deep Learning community."
  },
  {
    "objectID": "publications/nzoyem2025neural.html",
    "href": "publications/nzoyem2025neural.html",
    "title": "Neural Context Flows for Meta-Learning of Dynamical Systems",
    "section": "",
    "text": "Google Scholar"
  },
  {
    "objectID": "publications/nzoyem2025neural.html#citation-apa",
    "href": "publications/nzoyem2025neural.html#citation-apa",
    "title": "Neural Context Flows for Meta-Learning of Dynamical Systems",
    "section": "Citation (APA)",
    "text": "Citation (APA)\n\nNzoyem, R. D., Barton, D. A. W., & Deakin, T. (2025). Neural Context Flows for Meta-Learning of Dynamical Systems. The Thirteenth International Conference on Learning Representations. Retrieved from https://openreview.net/forum?id=8vzMLo8LDN"
  },
  {
    "objectID": "publications/nzoyem2025neural.html#abstract",
    "href": "publications/nzoyem2025neural.html#abstract",
    "title": "Neural Context Flows for Meta-Learning of Dynamical Systems",
    "section": "Abstract",
    "text": "Abstract\nNeural Ordinary Differential Equations (NODEs) often struggle to adapt to new dynamic behaviors caused by parameter changes in the underlying physical system, even when these dynamics are similar to previously observed behaviors. This problem becomes more challenging when the changing parameters are unobserved, meaning their value or influence cannot be directly measured when collecting data. To address this issue, we introduce Neural Context Flow (NCF), a robust and interpretable Meta-Learning framework that includes uncertainty estimation. NCF uses Taylor expansion to enable contextual self-modulation, allowing context vectors to influence dynamics from other domains while also modulating themselves. After establishing theoretical guarantees, we empirically test NCF and compare it to related adaptation methods. Our results show that NCF achieves state-of-the-art Out-of-Distribution performance on 5 out of 6 linear and non-linear benchmark problems. Through extensive experiments, we explore the flexible model architecture of NCF and the encoded representations within the learned context vectors. Our findings highlight the potential implications of NCF for foundational models in the physical sciences, offering a promising approach to improving the adaptability and generalization of NODEs in various scientific applications."
  },
  {
    "objectID": "projects/gnn4amg/index.html",
    "href": "projects/gnn4amg/index.html",
    "title": "AI4HPC",
    "section": "",
    "text": "How would you design a jet engine without ever building anything ? This project introduced a Graph Neural Network model to accelerate the Algebraic Multigrid (AMG) method for linear systems. The goal was to improve AMG at its crucial prolongation step. The focus is on large and sparse linear systems coming from high-fidelity fluid simulation of gas-turbine engines during operation. This work mainly used PyTorch, Deep Graph Library, and MATLAB.\nSoftware stack: - Deep Graph Library - PyTorch - JAX - MATLAB"
  },
  {
    "objectID": "projects/pdesim/index.html",
    "href": "projects/pdesim/index.html",
    "title": "UPDES",
    "section": "",
    "text": "This project emerged out of the frustration of there existing no differentiable PDE simulator capable of handling the diversity of problems out there. We identified Radial Basis Functions as a powerful and flexible mesh-free tool to control systems governed by partial differential equations, including non-linear PDEs like the Navier-Stokes equations. We showed that our discretise-then-optimise differentiable programming (DP) framework is superior to both the optimise-then-discretise direct-adjoint-looping (DAL) and the data-driven Physics-Informed Neural Network (PINN).\nSoftware stack: - JAX - GMSH - PyVista\nGitHub: üëâ Universal Partial Differential Equations Simulator"
  },
  {
    "objectID": "projects/arcticfloes/index.html",
    "href": "projects/arcticfloes/index.html",
    "title": "ArcticFloes",
    "section": "",
    "text": "The rapid shrinking of the Arctic ice cap these last decades is seen as one of the most striking manifestations of global warming. We investigated what happens when two or more ice floes collide, both in 1D and in 2D. We started by modeling an ice floe as a mass-spring-damper lattice, then we derived convergence guarantees with respect to its dynamics (percussion, fracture, etc.).\nSoftware stack: - Python Flask"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Home\n    Projects\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNeural Context Flow\n\n\n\nMachine Learning\n\n\nSimulation\n\n\nMeta Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUPDES\n\n\n\nModeling\n\n\nSimulation\n\n\nOptimal Control\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI4HPC\n\n\n\nAI\n\n\nHPC\n\n\nFluid Dynamics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArcticFloes\n\n\n\nModeling\n\n\nFunctional Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nœï-FEM\n\n\n\nModeling\n\n\nScientific Computing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVnetAgainstCancer\n\n\n\nAI\n\n\nSimulation\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Home\n    Publications\n  \n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    Weight-Space Linear Recurrent Neural Networks\n    Nzoyem, Keshtmand, Tsayem, Barton & Deakin\n    ArXiv\n    (2025)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation\n    Nzoyem, Barton & Deakin\n    Fourth Conference on Lifelong Learning Agents (CoLLAs)\n    (2025)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning\n    Nzoyem, Stevens, Sahota, Barton & Deakin\n    First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models @ ICLR\n    (2025)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    Neural Context Flows for Meta-Learning of Dynamical Systems\n    Nzoyem, Barton & Deakin\n    The Thirteenth International Conference on Learning Representations (ICLR)\n    (2025)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints\n    Nzoyem, Barton & Deakin\n    Proceedings of the SC'23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis\n    (2023)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    Accelerating Algebraic Multigrid Using Machine Learning\n    Nzoyem, Louw, McIntosh-Smith & Deakin\n    HPC Reaserach Group - University of Bristol (PhD Summer Project)\n    (2022)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    Fracturation de floes de glace par percussion dans un mod√®le granulaire\n    Nzoyem, Labb√© & Prud'homme\n    Laboratoire Jacques-Louis Lions - Sorbonne Universit√© (MSc Thesis)\n    (2021)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    Simulation 2D de l‚Äô√©quation du transfert radiatif et reconstruction de la densit√© par un r√©seau de neurones\n    Nzoyem, Franck, Navoret, Vigon & Prud'homme\n    Institut de Recherche Math√©matique Avanc√©e (MSc Thesis)\n    (2020)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Home\n    Blog\n  \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nA parallel theory of the causal mind\n\n\n\n\n\n\n\nCognitive Science\n\n\nParallel Computing\n\n\nDifferential Equations\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to my brand-new website !\n\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  }
]